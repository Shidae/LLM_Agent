{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some applications like chatbots, previous interactions are needed to keep the whole context of a conversation. Memory provide easy way to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi'), AIMessage(content='hello my dud')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# object stores sequence of messages in a conversation, keeping track of who said what\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# adds a message from the user to the history\n",
    "history.add_user_message(\"hi\")\n",
    "\n",
    "# adds a response from the AI to the history\n",
    "history.add_ai_message(\"hello my dud\")\n",
    "\n",
    "# returns list of all messages stored in the history object\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: hello my friend'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory language model that stores conversation history as a buffer \n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi\")\n",
    "memory.chat_memory.add_ai_message(\"hello my friend\")\n",
    "# {} passed as args, means no additional variables are being passed to modify \n",
    "# memory loading process\n",
    "# returns dict with key 'history': 'conversation history'\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ChatMessageHistory` is a simple storage of chat messages.\n",
    "- `ConversationBufferMemory` is a more complex object that stores chat history and provides method to load history in a format that easily used by llm in chains (e.g. ConversationChain) or agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello there! How are you doing today? I am currently running on a server located in a data center in California. The temperature in the data center is maintained at a constant 68 degrees Fahrenheit to ensure optimal performance of the servers. Did you know that the first data center was built in 1946 by IBM in New York? It was used to store punch cards for the US Census Bureau. Fascinating, isn't it?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI:  Hello there! How are you doing today? I am currently running on a server located in a data center in California. The temperature in the data center is maintained at a constant 68 degrees Fahrenheit to ensure optimal performance of the servers. Did you know that the first data center was built in 1946 by IBM in New York? It was used to store punch cards for the US Census Bureau. Fascinating, isn't it?\n",
      "Human: What is the Celcius equivalent of 68 degrees Fahrenheit?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The Celsius equivalent of 68 degrees Fahrenheit is 20 degrees Celsius. Did you know that the Celsius scale was created by Swedish astronomer Anders Celsius in 1742? He originally called it the \"centigrade\" scale because it was divided into 100 degrees. However, the name was changed to Celsius in 1948 to honor Celsius\\' contributions to science.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the Celcius equivalent of 68 degrees Fahrenheit?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI:  Hello there! How are you doing today? I am currently running on a server located in a data center in California. The temperature in the data center is maintained at a constant 68 degrees Fahrenheit to ensure optimal performance of the servers. Did you know that the first data center was built in 1946 by IBM in New York? It was used to store punch cards for the US Census Bureau. Fascinating, isn't it?\n",
      "Human: What is the Celcius equivalent of 68 degrees Fahrenheit?\n",
      "AI:  The Celsius equivalent of 68 degrees Fahrenheit is 20 degrees Celsius. Did you know that the Celsius scale was created by Swedish astronomer Anders Celsius in 1742? He originally called it the \"centigrade\" scale because it was divided into 100 degrees. However, the name was changed to Celsius in 1948 to honor Celsius' contributions to science.\n",
      "Human: What do you mean something is divided by 100? 68 Fahrenheit divided by 100?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Oh, I apologize for the confusion. I was referring to the Celsius scale being divided into 100 degrees. In Fahrenheit, 68 degrees is equivalent to 20 degrees Celsius. The Fahrenheit scale was created by German physicist Daniel Gabriel Fahrenheit in 1724 and is based on a scale of 180 degrees, with 32 degrees being the freezing point of water and 212 degrees being the boiling point.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What do you mean something is divided by 100? 68 Fahrenheit divided by 100?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Summary Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inputs get long, we might not want to send the whole conversation, but rather a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "review = \"I ordered a menu of soy garlic chicken and spicy chicken ala carte for RM60. \\\n",
    "The pizza was delivered on time and was still hot when I received it. \\\n",
    "However, no receipt was given and I couldn't figure out if the order is correct.\\\n",
    "When eating the spicy chicken, soy garlic chicken was not included in the delivery. \\\n",
    "I took the picture of my order and made a complaint. \\\n",
    "Within 30 minutes, I get partial refund. \\\n",
    "I am thankful for the quick response from the support and hope they do better in the future.\"\n",
    "\n",
    "# to store and manage conversation history\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "# first, add context at the start of conversation\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello, how can I help you today?\"},\n",
    "    {\"output\": \"Could you analyze a review for me?\"}, # user ask ai to summarize review\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Sure, I'd be happy to. Could you provide the review?\"},\n",
    "    {\"output\": f\"{review}\"}, # user\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human offers to help the AI and the AI asks for a review to be analyzed. The human agrees and the AI provides a review of a food delivery experience. The review mentions a missing item and a complaint made, resulting in a partial refund and a positive response from customer support.\n",
      "Human: Thank you very much!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" You're welcome! I'm always happy to help. So, the review I would like you to analyze is from a customer who ordered food delivery from a popular restaurant. They mentioned that one item was missing from their order and they made a complaint to customer support. As a result, they received a partial refund and were very pleased with the response from customer support.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates object that's ready to be used for conversation\n",
    "# predict() to start conversation\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"Thank you very much!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: \\nThe human offers to help the AI and the AI asks for a review to be analyzed. The human agrees and the AI provides a review of a food delivery experience. The review mentions a missing item and a complaint made, resulting in a partial refund and a positive response from customer support.\\nHuman: Thank you very much!\\nAI:  You're welcome! I'm always happy to help. So, the review I would like you to analyze is from a customer who ordered food delivery from a popular restaurant. They mentioned that one item was missing from their order and they made a complaint to customer support. As a result, they received a partial refund and were very pleased with the response from customer support.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "The human offers to help the AI and the AI asks for a review to be analyzed. The human agrees and the AI provides a review of a food delivery experience. The review mentions a missing item and a complaint made, resulting in a partial refund and a positive response from customer support.\n",
      "Human: Thank you very much!\n",
      "AI:  You're welcome! I'm always happy to help. So, the review I would like you to analyze is from a customer who ordered food delivery from a popular restaurant. They mentioned that one item was missing from their order and they made a complaint to customer support. As a result, they received a partial refund and were very pleased with the response from customer support.\n"
     ]
    }
   ],
   "source": [
    "# Print the conversation summary\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human offers to help the AI and the AI asks for a review to be analyzed. The human agrees and the AI provides a review of a food delivery experience. The review mentions a missing item and a complaint made, resulting in a partial refund and a positive response from customer support.\n",
      "Human: Thank you very much!\n",
      "AI:  You're welcome! I'm always happy to help. So, the review I would like you to analyze is from a customer who ordered food delivery from a popular restaurant. They mentioned that one item was missing from their order and they made a complaint to customer support. As a result, they received a partial refund and were very pleased with the response from customer support.\n",
      "Human: The problem has settled. You may now close it.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Thank you for letting me know. I will make a note to close this issue in my system. Is there anything else I can assist you with?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"The problem has settled. You may now close it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
