{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n    \"sentiment\": \"negative\",\\n    \"subject\": \"food\"\\n}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "# Create prompt template using template string\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "# Create a runnable sequence using the prompt and the LLM\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Use the runnable sequence to make a prediction\n",
    "output = chain.invoke({\"input\": \"I get briyani for lunch today, it was a lot of carbs.\"})\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to pass the output from one model to a another model. This can be done using with different SequentialChains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\"\"\n",
    "You are a helpful bot that creates a 'thank you' response text. \n",
    "If customers are unsatisfied, offer them a real world assitant to talk to. \n",
    "You will get a sentiment and subject as into and evaluate. \n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "review_template = PromptTemplate(input_variables=[\"input\"], template=response_template)\n",
    "review_chain = review_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThank you for bringing this to our attention. We apologize for any dissatisfaction you may have experienced with our food. We take all feedback seriously and will work to improve our offerings. If you would like to speak with a real world assistant about your experience, please let us know and we will arrange for someone to contact you. We value your feedback and hope to have the opportunity to make it right.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the chain replaces the SimpleSequential Chain\n",
    "overall_chain = chain | review_chain\n",
    "\n",
    "output = overall_chain.invoke({\"input\": \"I ordered malaysian Rendang however it was crispy.\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'malaysian rendang',\n",
       " 'experience': 'Traditionally, the chicken is not supposed to be crispy!',\n",
       " 'review': '\\n\\nI recently ordered the Malaysian rendang from your restaurant and I must say, I was blown away by the flavors and authenticity of the dish. The rich and aromatic spices used in the rendang were perfectly balanced and created a burst of flavors in every bite.\\n\\nHowever, I did notice that the chicken in the dish was crispy, which is not traditionally how rendang is prepared. While it did not affect the taste of the dish, I wanted to bring it to your attention as it may be a concern for other customers who are looking for an authentic rendang experience.\\n\\nDespite this small discrepancy, I thoroughly enjoyed my meal and would highly recommend your restaurant to anyone looking for delicious Malaysian cuisine. The service was also excellent and the staff were very friendly and accommodating.\\n\\nI appreciate the effort put into creating such a delicious dish and I will definitely be returning to try more of your menu. Thank you for a wonderful dining experience.',\n",
       " 'comment': \"\\n\\nAfter my previous review, I wanted to give an update on my experience at your restaurant. I am happy to say that I have visited again and the rendang dish was prepared perfectly this time. The chicken was tender and the flavors were spot on. It shows that you take customer feedback seriously and strive for excellence in your dishes.\\n\\nI also had the opportunity to try some other dishes on your menu and they were all equally delicious. The nasi lemak and satay were some of the best I've had outside of Malaysia. Your restaurant truly captures the essence of Malaysian cuisine.\\n\\nI would like to commend your staff for their exceptional service once again. They were attentive and made sure we had a great dining experience.\\n\\nOverall, I am extremely satisfied with my visits to your restaurant and will continue to recommend it to others. Keep up the great work and I look forward to trying more of your dishes in the future.\",\n",
       " 'summary': '\\n\\nAuthentic and flavorful Malaysian rendang dish with crispy chicken, but overall highly recommended for delicious cuisine and excellent service.',\n",
       " 'macedonian_translation': '\\n\\nАутентично и вкусно малезиско ренданг јадење со хрупкаво пилешко месо, но во целост силно препорачливо за вкусна кујна и одлична услуга.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a review (for me to the restaurant) given a dish name and the experience.\n",
    "prompt_review = PromptTemplate.from_template(\n",
    "    template=\"You ordered {dish_name} and your experience was {experience}. Write a review: \"\n",
    ")\n",
    "chain_review = LLMChain(llm=llm, prompt=prompt_review, output_key=\"review\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up comment given the restaurant review.\n",
    "prompt_comment = PromptTemplate.from_template(\n",
    "    template=\"Given the restaurant review: {review}, write a follow-up comment: \"\n",
    ")\n",
    "chain_comment = LLMChain(llm=llm, prompt=prompt_comment, output_key=\"comment\")\n",
    "\n",
    "# This is an LLMChain to summarize a review.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"Summarise the review in one short sentence: \\n\\n {review}\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "# This is an LLMChain to translate a summary into Macedonian.\n",
    "prompt_translation = PromptTemplate.from_template(\n",
    "    template=\"Translate the summary to macedonian: \\n\\n {summary}\"\n",
    ")\n",
    "chain_translation = LLMChain(\n",
    "    llm=llm, prompt=prompt_translation, output_key=\"macedonian_translation\"\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_review, chain_comment, chain_summary, chain_translation],\n",
    "    input_variables=[\"dish_name\", \"experience\"],\n",
    "    output_variables=[\"review\", \"comment\", \"summary\", \"macedonian_translation\"],\n",
    ")\n",
    "\n",
    "overall_chain({\"dish_name\": \"malaysian rendang\", \"experience\": \"Traditionally, the chicken is not supposed to be crispy!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is an LLMChain to write a review given a dish name and the experience.\n",
    "# prompt_review = PromptTemplate.from_template(\n",
    "#     template=\"You ordered {dish_name} and your experience was {experience}. Write a review: \"\n",
    "# )\n",
    "# chain_review = prompt_review | llm\n",
    "\n",
    "# # This is an LLMChain to write a follow-up comment given the restaurant review.\n",
    "# prompt_comment = PromptTemplate.from_template(\n",
    "#     template=\"Given the restaurant review: {{review}}, write a follow-up comment: \"\n",
    "# )\n",
    "# chain_comment = prompt_comment | llm\n",
    "\n",
    "# # This is an LLMChain to summarize a review.\n",
    "# prompt_summary = PromptTemplate.from_template(\n",
    "#     template=\"Summarise the review in one short sentence: \\n\\n {{review}}\"\n",
    "# )\n",
    "# chain_summary = prompt_summary | llm\n",
    "\n",
    "# # This is an LLMChain to translate a summary into German.\n",
    "# prompt_translation = PromptTemplate.from_template(\n",
    "#     template=\"Translate the summary to macedonian: \\n\\n {summary}\"\n",
    "# )\n",
    "# chain_translation = prompt_translation | llm\n",
    "\n",
    "# # Chain them together sequentially\n",
    "# overall_chain = chain_review | chain_comment | chain_summary | chain_translation\n",
    "\n",
    "# # Run the overall chain with the input data\n",
    "# result = overall_chain.invoke({\n",
    "#     \"dish_name\": \"malaysian rendang\",\n",
    "#     \"experience\": \"It was crispy!\"\n",
    "# })\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x00000258CDC5DDF0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000258CDC5E330>, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'neutral': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x00000258CDC5DDF0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000258CDC5E330>, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'negative': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x00000258CDC5DDF0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000258CDC5E330>, openai_api_key=SecretStr('**********'), openai_proxy=''))}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# destination chains\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for analyzing neutral sentiments\",\n",
    "        \"prompt_template\": neutral_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "# destinations where the input text eventually will be processed\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    # creates a chain by combining the prompt template with an LLM\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain # stores the chain in the dict, using name as the key\n",
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "neutral: {'input': 'I ordered korean chicken.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nBased on the text provided, it can be inferred that the person ordered Korean chicken. This statement does not provide any positive or negative opinions about the dish, just a neutral fact.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates list of strings of dictionary, combine name and description of the prompt\n",
    "# e.g. [\"positive : Good for analyzing positive sentiments\",...]\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "\n",
    "# Joins all the strings in the destinations into a single string, remove list\n",
    "# each item is separated by a newline character\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "# format the template by inserting the individual destinations into it\n",
    "# give access to router in selecting the appropriate chain (positive, neutral, negative)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# creates PromptTemplate object using the formatted router_template\n",
    "# uses RouterOutputParser to parse the output\n",
    "router_prompt = PromptTemplate(template=router_template, input_variables=[\"input\"], output_parser=RouterOutputParser())\n",
    "\n",
    "# creates an object to DECIDE which destination chain to use based on the input\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# creates object which combines router chain, destination chains, default chain (if can't decide)\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=destination_chains[\"neutral\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"I ordered korean chicken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n> Entering new MultiPromptChain chain...\\nneutral: {'input': 'I ordered korean chicken.'} # format from MULTI_PROMPT_ROUTER_TEMPLATE\\n> Finished chain.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "> Entering new MultiPromptChain chain...\n",
    "neutral: {'input': 'I ordered korean chicken.'} # format from MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "> Finished chain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive: Good for analyzing positive sentiments',\n",
       " 'neutral: Good for analyzing neutral sentiments',\n",
       " 'negative: Good for analyzing negative sentiments']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive: Good for analyzing positive sentiments\\nneutral: Good for analyzing neutral sentiments\\nnegative: Good for analyzing negative sentiments'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str = \"\\n\".join(destinations)\n",
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
