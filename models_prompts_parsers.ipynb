{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import OpenAI as LangChainOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load env variables\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI model instance using LangChain\n",
    "llm = LangChainOpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x0000023B4047F650>, async_client=<openai.resources.completions.AsyncCompletions object at 0x0000023B404B5430>, openai_api_key='sk-Hjsnww-7kJuSkfrRZb7aMY1ADeZh5fP6B0bvnd-0WwT3BlbkFJ7cx1ZsmSgUsimu7lA0v05xL05hOe0TgUQQMfzJrA4A', openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the  model to generate a joke\n",
    "joke = llm(\"Tell me a joke\")\n",
    "joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for evaluating sentiment and subject\n",
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using LangChain's ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain with the prompt and the model\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOutput: \\n{\\n    \"sentiment\": \"neutral\",\\n    \"subject\": \"technology\"\\n}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment and subject of the input text\n",
    "output = chain.predict(input=\"I created a AI automated Chatbot today and it was okay\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentiment: neutral\\nsubject: technology\\n\\n{\\n    \"sentiment\": \"neutral\",\\n    \"subject\": \"technology\"\\n}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative new version\n",
    "# this creates a runnable sequence that first processes the input through the prompt_template\n",
    "# and then passes the result to the llm\n",
    "chain = prompt_template | llm\n",
    "# invoke method uses this chain\n",
    "output = chain.invoke({\"input\": \"I created an AI automated Chatbot today and it was okay.\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World example with ResponseSchema, Templates, Chains and OutputParsers\n",
    "`There were two issues with the output: It contains text and the output is just a string instead of a dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define response schema (structured output) for parsing\n",
    "# key and value pair for sentiment, subjectm and price schema\n",
    "# description defined as // in the format_instructions\n",
    "sentiment_schema = ResponseSchema(\n",
    "    name=\"sentiment\",\n",
    "    description=\"Is the text positive, neutral or negative? Only provide these words.\"\n",
    ")\n",
    "\n",
    "subject_schema = ResponseSchema(\n",
    "    name=\"subject\",\n",
    "    description=\"What subject is the text about? Use exactly one word.\"\n",
    ")\n",
    "price_schema = ResponseSchema(\n",
    "    name=\"price\",\n",
    "    description=\"How expensive was the product? Use None if no price was provided in the text.\"\n",
    ")\n",
    "# list\n",
    "response_schemas = [sentiment_schema, subject_schema, price_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the structured output parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"sentiment\": string  // Is the text positive, neutral or negative? Only provide these words.\\n\\t\"subject\": string  // What subject is the text about? Use exactly one word.\\n\\t\"price\": string  // How expensive was the product? Use None if no price was provided in the text.\\n}\\n```'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get format instructions for the parser\n",
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template with format instructions included\n",
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "price: How expensive was the product? Use None if no price was provided in the text.\n",
    "\n",
    "Just return the JSON, do not add ANYTHING, NO INTERPRETATION!\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escape the format_instructions string\n",
    "# treat {} as literal text for e.g. {'sentiment': 'positive', 'subject': 'food', 'price': 'None'}\n",
    "escaped_format_instructions = format_instructions.replace('{', '{{').replace('}', '}}')\n",
    "\n",
    "# Format the messages with input and format instructions\n",
    "messages = prompt.format_messages(\n",
    "    input=\"I ordered Chicken and Coleslaw from Texas it was spicy.\",\n",
    "    format_instructions=escaped_format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'positive', 'subject': 'food', 'price': 'None'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the OpenAI chat model with zero temperature for deterministic output\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "response = chat(messages)\n",
    "parsed_output = parser.parse(response.content)\n",
    "parsed_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
