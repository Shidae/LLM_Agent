{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store to store the vectors in a database\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to serialize database and to store into our file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Laden Sie die Umgebungsvariablen aus der .env-Datei\n",
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use data with an LLM, documents must first be loaded into a vector database. The first step is to load them into memory via a loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    # ** matches any number of dir and subdir, /* matches any file with .txt\n",
    "    # use TextLoader to process each file\n",
    "    \"./FAQ\", glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True\n",
    ")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts are not loaded 1:1 into a database, but in pieces called chunks. You can define the chunk size and the overlap between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FAQ\\\\General.txt'}, page_content='Q: What are the hours of operation for your restaurant?\\nA: Our restaurant is open from 11 a.m. to 10 p.m. from Monday to Saturday. On Sundays, we open at 12 p.m. and close at 9 p.m.\\n\\nQ: What type of cuisine does your restaurant serve?\\nA: Our restaurant specializes in contemporary American cuisine with an emphasis on local and sustainable ingredients.'),\n",
       " Document(metadata={'source': 'FAQ\\\\General.txt'}, page_content='Q: Do you offer vegetarian or vegan options?\\nA: Yes, we have a range of dishes to cater to vegetarians and vegans. Please let our staff know about any dietary restrictions you have when you order.'),\n",
       " Document(metadata={'source': 'FAQ\\\\Health.txt'}, page_content=\"Q: What are the ingredients in your gluten-free options?\\nA: Our gluten-free dishes are prepared using a variety of ingredients that don't contain gluten. Some options include our Quinoa Salad and our Grilled Chicken with Roasted Vegetables.\"),\n",
       " Document(metadata={'source': 'FAQ\\\\Health.txt'}, page_content='Q: What steps is your restaurant taking to ensure safety amid the ongoing pandemic?\\nA: We adhere to strict health and safety protocols, including regular sanitization, maintaining physical distance between tables, and providing hand sanitizers for customers. We also offer contactless pickup and delivery options.'),\n",
       " Document(metadata={'source': 'FAQ\\\\Health.txt'}, page_content=\"Q: Can I request alterations to a dish due to allergies?\\nA: Absolutely, we strive to accommodate all of our customers' needs. Please inform our staff about any allergies you have, and we'll do our best to modify the dish accordingly.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split text into smaller chunks recursively, first by /n/n, then by /n and so on.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # each chunk is 500 chars long\n",
    "    chunk_size = 500, # ~125 tokens\n",
    "    # character overlap between consecutive chunks for continuity and maintain context\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents[:5] # first 5 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts are not stored as text in the database, but as vector representations. Embeddings are a type of word representation that represents the semantic meaning of words in a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Vectors into VectorDB (FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, vectors can be stored in the database. The DB can be stored as .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "import faiss\n",
    "\n",
    "# separates the FAISS index (contains vector data) from the document store (actual text data)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(vectorstore.index, \"faiss_index.pkl\")\n",
    "\n",
    "# Save the documents and the index_to_docstore_id mapping\n",
    "# rb and wb reads and writes in binary mode which does not handle as plain text\n",
    "with open(\"faiss_store.pkl\", \"wb\") as f:\n",
    "    pickle.dump((vectorstore.docstore._dict, vectorstore.index_to_docstore_id), f)\n",
    "\n",
    "# When you want to load it back\n",
    "index = faiss.read_index(\"faiss_index.pkl\")\n",
    "with open(\"faiss_store.pkl\", \"rb\") as f:\n",
    "    docstore, index_to_docstore_id = pickle.load(f)\n",
    "\n",
    "# Recreate the embeddings object, does not re-embed the docs\n",
    "# before, the embeddings has elements (thread locks) that can't be pickled\n",
    "# so it couldn't be saved directly\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Recreate the FAISS object\n",
    "loaded_vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an LLM, it is possible to give it an identity before a conversation or to define how question and answer should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a veteran restaurant manager for our restaurant.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer here:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    # input variables are the {text} in the template\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With chain classes you can easily influence the behaviour of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: Of course, we highly recommend making a reservation to ensure we can accommodate you. Please call our restaurant at [phone number] or visit our website to make a reservation online. Thank you!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.llms import OpenAI # outdated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# create llm\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create the retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    # \"stuff\" retrieves all documents from vectorstore all at once\n",
    "    # good for small datasets\n",
    "    # alternative: \"map_reduce\", \"refine\"\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    # passing custom prompt instead of generic prompt\n",
    "    chain_type_kwargs={\"prompt\":PROMPT},\n",
    ")\n",
    "query = \"I would like to make a reservation.\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example just shown, each request stands alone. A great strength of an LLM, is that it can take the entire chat history into account when responding. For this, a chat history must be built up from the different questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# simple conversation memory object, to remember previous exchanges in a conversation\n",
    "memory = ConversationBufferMemory(\n",
    "    # conversation history will be stored under \"chat_history\" key\n",
    "    # True returns the history a list of messages, with \"human\" and \"ai\" keys, otherwise return single string\n",
    "    # expects the chain to produce its main output under \"answer\" key, to access later\n",
    "    memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Memory in Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory class can now easily be used in a chain. This is recognizable, e.g. by the fact that when once speaks of \"it\", the bot understands the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Of course! How many people will be in your party and what time would you like to make the reservation for?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-4\" if you have access\n",
    "    openai_api_key=API_KEY\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain_kwargs={\"prompt\":PROMPT},\n",
    ")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"I'd like to make a reservation for tonight.\"\n",
    "result = qa({\"question\": query})\n",
    "print(result['answer']) # print answer only instead of the whole history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'There are 8 people in total and we would like to request a seat for a baby. At 14 Uhr',\n",
       " 'chat_history': [HumanMessage(content=\"I'd like to make a reservation for tonight.\"),\n",
       "  AIMessage(content='A: Of course! How many people will be in your party and what time would you like to make the reservation for?'),\n",
       "  HumanMessage(content='There are 8 people in total and we would like to request a seat for a baby. At 14 Uhr'),\n",
       "  AIMessage(content='Customer: I would like to make a reservation for 6 people at 7 p.m. on Saturday.')],\n",
       " 'answer': 'Customer: I would like to make a reservation for 6 people at 7 p.m. on Saturday.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(input=\"There are 8 people in total and we would like to request a seat for a baby. At 14 Uhr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"There are 8 people in total and we would like to request a seat for a baby. At 14 Uhr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, we would be happy to accommodate your reservation for 8 people, including a seat for a baby, at 2 p.m. Please provide us with your name and contact information so we can confirm the reservation. Thank you!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa.invoke({\"question\": query})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
